{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation with Amazon SageMaker Processing and Dask\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Dask are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Dask in a managed SageMaker environment to run our preprocessing workload.\n",
    "\n",
    "### What is Dask Distributed?\n",
    "Dask.distributed: is a lightweight and open source library for distributed computing in Python. It is also a centrally managed, distributed, dynamic task scheduler. It is also a centrally managed, distributed, dynamic task scheduler. Dask has three main components:\n",
    "\n",
    "**dask-scheduler process:** coordinates the actions of several workers. The scheduler is asynchronous and event-driven, simultaneously responding to requests for computation from multiple clients and tracking the progress of multiple workers.\n",
    "\n",
    "**dask-worker processes:** Which are spread across multiple machines and the concurrent requests of several clients.\n",
    "\n",
    "**dask-client process:** which is is the primary entry point for users of dask.distributed\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-overview.svg\">\n",
    "\n",
    "source: https://docs.dask.org/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective:-predict-the-age-of-an-Abalone-from-its-physical-measurement)\n",
    "1. [Setup](#Setup)\n",
    "1. [Using Amazon SageMaker Processing to execute a Dask Job](#Using-Amazon-SageMaker-Processing-to-execute-a-Dask-Job)\n",
    "  1. [Downloading dataset and uploading to S3](#Downloading-dataset-and-uploading-to-S3)\n",
    "  1. [Build a Dask container for running the preprocessing job](#Build-a-Dask-container-for-running-the-preprocessing-job)\n",
    "  1. [Run the preprocessing job using Amazon SageMaker Processing](#Run-the-preprocessing-job-using-Amazon-SageMaker-Processing)\n",
    "    1. [Inspect the preprocessed dataset](#Inspect-the-preprocessed-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "prefix = \"sagemaker/dask-preprocess-demo\"\n",
    "input_prefix = prefix + \"/input/raw/census\"\n",
    "input_preprocessed_prefix = prefix + \"/input/preprocessed/census\"\n",
    "model_prefix = prefix + \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon SageMaker Processing to execute a Dask job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset and uploading to Amazon Simple Storage Service (Amazon S3)\n",
    "\n",
    "The dataset used here is the Census-Income KDD Dataset. The first step are to select features, clean the data, and turn the data into features that the training algorithm can use to train a binary classification model which can then be used to predict whether rows representing census responders have an income greater or less than $50,000. In this example, we will use Dask distributed to preprocess and transform the data to make it ready for the training process. In the next section, you download from the bucket below then upload to your own bucket so that Amazon SageMaker can access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv to ./census-income.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-571572254388/sagemaker/dask-preprocess-demo/input/raw/census/census-income.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "region = sagemaker_session.boto_region_name\n",
    "input_data = 's3://sagemaker-sample-data-{}/processing/census/census-income.csv'.format(region)\n",
    "!aws s3 cp $input_data .\n",
    "\n",
    "# Uploading the training data to S3\n",
    "sagemaker_session.upload_data(path='census-income.csv', bucket=bucket, key_prefix=input_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dask container for running the preprocessing job\n",
    "\n",
    "An example Dask container is included in the `./container` directory of this example. The container handles the bootstrapping of Dask Scheduler and mapping each instance to a Dask Worke. At a high level the container provides:\n",
    "\n",
    "* A set of default worker/scheduler configurations\n",
    "* A bootstrapping script for configuring and starting up  scheduler/worker nodes\n",
    "* Starting dask cluster from all the workers including the scheduler node\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed dask application that performs our dataset preprocessing.\n",
    "\n",
    "### Build the example Dask container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/feature_transformation_with_sagemaker_processing_dask_2020-10-29/container\n",
      "Sending build context to Docker daemon  9.216kB\n",
      "Step 1/21 : FROM continuumio/miniconda3:4.7.12\n",
      " ---> 406f2b43ea59\n",
      "Step 2/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 34d820e45856\n",
      "Step 3/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> 5cb53fa2f772\n",
      "Step 4/21 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> debd214d18e1\n",
      "Step 5/21 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> b8c51f30d5d9\n",
      "Step 6/21 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 720060d22c1c\n",
      "Step 7/21 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 47b2c8fbcc93\n",
      "Step 8/21 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 4d0dc2de29c9\n",
      "Step 9/21 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 1fceea2be76b\n",
      "Step 10/21 : RUN conda install --yes     -c conda-forge     python==3.8     python-blosc     cytoolz     dask==2.16.0     distributed==2.16.0     lz4     nomkl     numpy==1.18.1     pandas==1.0.1     tini==0.18.0     && conda clean -tipsy     && find /opt/conda/ -type f,l -name '*.a' -delete     && find /opt/conda/ -type f,l -name '*.pyc' -delete     && find /opt/conda/ -type f,l -name '*.js.map' -delete     && find /opt/conda/lib/python*/site-packages/bokeh/server/static -type f,l -name '*.js' -not -name '*.min.js' -delete     && rm -rf /opt/conda/pkgs\n",
      " ---> Using cache\n",
      " ---> c7fc75d20ed5\n",
      "Step 11/21 : RUN pip install dask-ml boto3\n",
      " ---> Running in 0988e4a7139b\n",
      "Collecting dask-ml\n",
      "  Downloading dask_ml-1.7.0-py3-none-any.whl (141 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.16.21-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (1.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from dask-ml) (20.4)\n",
      "Collecting scikit-learn>=0.23\n",
      "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (2.16.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (1.18.1)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (2.16.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.5.4-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "Collecting multipledispatch>=0.4.9\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting dask-glm>=0.2.0\n",
      "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.51.2-cp38-cp38-manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting botocore<1.20.0,>=1.19.21\n",
      "  Downloading botocore-1.19.21-py2.py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24.2->dask-ml) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24.2->dask-ml) (2020.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from packaging->dask-ml) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->dask-ml) (2.4.7)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (1.6.0)\n",
      "Requirement already satisfied: tornado>=6.0.3; python_version >= \"3.8\" in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (6.0.4)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (2.2.2)\n",
      "Requirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (5.7.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (49.6.0.post20201009)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: click>=6.6 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (7.1.2)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (1.6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (0.11.1)\n",
      "Requirement already satisfied: zict>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (2.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (5.1.2)\n",
      "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /opt/conda/lib/python3.8/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.1.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /opt/conda/lib/python3.8/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.8.4)\n",
      "Collecting llvmlite<0.35,>=0.34.0.dev0\n",
      "  Downloading llvmlite-0.34.0-cp38-cp38-manylinux2010_x86_64.whl (24.6 MB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.21->boto3) (1.25.11)\n",
      "Requirement already satisfied: heapdict in /opt/conda/lib/python3.8/site-packages (from zict>=0.1.3->distributed>=2.4.0->dask-ml) (1.0.1)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.8/site-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[array,dataframe]>=2.4.0->dask-ml) (0.2.0)\n",
      "Installing collected packages: joblib, scipy, threadpoolctl, scikit-learn, multipledispatch, dask-glm, llvmlite, numba, dask-ml, jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.21 botocore-1.19.21 dask-glm-0.2.0 dask-ml-1.7.0 jmespath-0.10.0 joblib-0.17.0 llvmlite-0.34.0 multipledispatch-0.6.0 numba-0.51.2 s3transfer-0.3.3 scikit-learn-0.23.2 scipy-1.5.4 threadpoolctl-2.1.0\n",
      "Removing intermediate container 0988e4a7139b\n",
      " ---> d33361cbf165\n",
      "Step 12/21 : RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      " ---> Running in 655559c8ff0c\n",
      "\u001b[91m--2020-11-19 19:03:13--  https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      "\u001b[0m\u001b[91mResolving github.com (github.com)... \u001b[0m\u001b[91m140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m302 Found\n",
      "\u001b[0m\u001b[91mLocation: https://github-production-release-asset-2e65be.s3.amazonaws.com/40563188/9880c3f8-8ef4-11e6-903c-6ceae7a11e13?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201119T190313Z&X-Amz-Expires=300&X-Amz-Signature=8a0e80b9509e117d9257a79ee936d3b51c1eb0c31f786f0c9f33de452d6bf56e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=40563188&response-content-disposition=attachment%3B%20filename%3Ddumb-init_1.2.0_amd64&response-content-type=application%2Foctet-stream [following]\n",
      "\u001b[0m\u001b[91m--2020-11-19 19:03:13--  https://github-production-release-asset-2e65be.s3.amazonaws.com/40563188/9880c3f8-8ef4-11e6-903c-6ceae7a11e13?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201119%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201119T190313Z&X-Amz-Expires=300&X-Amz-Signature=8a0e80b9509e117d9257a79ee936d3b51c1eb0c31f786f0c9f33de452d6bf56e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=40563188&response-content-disposition=attachment%3B%20filename%3Ddumb-init_1.2.0_amd64&response-content-type=application%2Foctet-stream\n",
      "\u001b[0m\u001b[91mResolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... \u001b[0m\u001b[91m52.216.163.123\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.163.123|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m200 OK\n",
      "\u001b[0m\u001b[91mLength: 46400 (45K) [application/octet-stream]\n",
      "Saving to: ‘/usr/local/bin/dumb-init’\n",
      "\u001b[0m\u001b[91m\n",
      "     0K .......\u001b[0m\u001b[91m... .........\u001b[0m\u001b[91m. ....\u001b[0m\u001b[91m...... ...\u001b[0m\u001b[91m.\u001b[0m\u001b[91m...... .....     100% 23.5M=0.002s\n",
      "\n",
      "\u001b[0m\u001b[91m2020-11-19 19:03:13 (23.5 MB/s) - ‘/usr/local/bin/dumb-init’ saved [46400/46400]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mRemoving intermediate container 655559c8ff0c\n",
      " ---> 664e6f6f9e13\n",
      "Step 13/21 : RUN chmod +x /usr/local/bin/dumb-init\n",
      " ---> Running in 4593f5a8aa8a\n",
      "Removing intermediate container 4593f5a8aa8a\n",
      " ---> e662b3c20d7b\n",
      "Step 14/21 : RUN apt-get update\n",
      " ---> Running in 51f230f8ff87\n",
      "Get:1 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]\n",
      "Get:2 http://deb.debian.org/debian buster InRelease [121 kB]\n",
      "Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]\n",
      "Get:4 http://security.debian.org/debian-security buster/updates/main amd64 Packages [248 kB]\n",
      "Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7906 kB]\n",
      "Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [7856 B]\n",
      "Fetched 8401 kB in 2s (5219 kB/s)\n",
      "Reading package lists...\n",
      "Removing intermediate container 51f230f8ff87\n",
      " ---> 139b70547ac8\n",
      "Step 15/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Running in be28c21b0e1c\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.64.0-4+deb10u1).\n",
      "python-dev is already the newest version (2.7.16-1).\n",
      "python3-pip is already the newest version (18.1-5).\n",
      "python-psutil is already the newest version (5.5.1-1).\n",
      "python3-setuptools is already the newest version (40.8.0-1).\n",
      "python3 is already the newest version (3.7.3-1).\n",
      "python3-dev is already the newest version (3.7.3-1).\n",
      "unzip is already the newest version (6.0-23+deb10u1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
      "Removing intermediate container be28c21b0e1c\n",
      " ---> e4b2c5a47f20\n",
      "Step 16/21 : RUN conda install --yes s3fs -c conda-forge\n",
      " ---> Running in 81d75e78da03\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - s3fs\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    aiobotocore-1.1.2          |             py_0          39 KB  conda-forge\n",
      "    aiohttp-3.7.3              |   py38h25fe258_0         619 KB  conda-forge\n",
      "    aioitertools-0.7.1         |     pyhd8ed1ab_0          19 KB  conda-forge\n",
      "    async-timeout-3.0.1        |          py_1000          11 KB  conda-forge\n",
      "    attrs-20.3.0               |     pyhd3deb0d_0          41 KB  conda-forge\n",
      "    botocore-1.17.44           |     pyh9f0ad1d_0         4.0 MB  conda-forge\n",
      "    ca-certificates-2020.11.8  |       ha878542_0         145 KB  conda-forge\n",
      "    certifi-2020.11.8          |   py38h578d9bd_0         150 KB  conda-forge\n",
      "    conda-4.9.2                |   py38h578d9bd_0         3.0 MB  conda-forge\n",
      "    docutils-0.15.2            |           py38_0         735 KB  conda-forge\n",
      "    jmespath-0.10.0            |     pyh9f0ad1d_0          21 KB  conda-forge\n",
      "    multidict-4.7.5            |   py38h1e0a361_2          71 KB  conda-forge\n",
      "    s3fs-0.5.1                 |             py_0          22 KB  conda-forge\n",
      "    typing-extensions-3.7.4.3  |                0           8 KB  conda-forge\n",
      "    wrapt-1.12.1               |   py38h1e0a361_1          47 KB  conda-forge\n",
      "    yarl-1.6.3                 |   py38h25fe258_0         142 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         9.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  aiobotocore        conda-forge/noarch::aiobotocore-1.1.2-py_0\n",
      "  aiohttp            conda-forge/linux-64::aiohttp-3.7.3-py38h25fe258_0\n",
      "  aioitertools       conda-forge/noarch::aioitertools-0.7.1-pyhd8ed1ab_0\n",
      "  async-timeout      conda-forge/noarch::async-timeout-3.0.1-py_1000\n",
      "  attrs              conda-forge/noarch::attrs-20.3.0-pyhd3deb0d_0\n",
      "  botocore           conda-forge/noarch::botocore-1.17.44-pyh9f0ad1d_0\n",
      "  docutils           conda-forge/linux-64::docutils-0.15.2-py38_0\n",
      "  jmespath           conda-forge/noarch::jmespath-0.10.0-pyh9f0ad1d_0\n",
      "  multidict          conda-forge/linux-64::multidict-4.7.5-py38h1e0a361_2\n",
      "  s3fs               conda-forge/noarch::s3fs-0.5.1-py_0\n",
      "  typing-extensions  conda-forge/noarch::typing-extensions-3.7.4.3-0\n",
      "  wrapt              conda-forge/linux-64::wrapt-1.12.1-py38h1e0a361_1\n",
      "  yarl               conda-forge/linux-64::yarl-1.6.3-py38h25fe258_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2020.6.20-hecda079_0 --> 2020.11.8-ha878542_0\n",
      "  certifi                          2020.6.20-py38h924ce5b_2 --> 2020.11.8-py38h578d9bd_0\n",
      "  conda                                4.9.0-py38h924ce5b_1 --> 4.9.2-py38h578d9bd_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2020 | 145 KB    | ########## | 100% \n",
      "botocore-1.17.44     | 4.0 MB    | ########## | 100% \n",
      "s3fs-0.5.1           | 22 KB     | ########## | 100% \n",
      "multidict-4.7.5      | 71 KB     | ########## | 100% \n",
      "async-timeout-3.0.1  | 11 KB     | ########## | 100% \n",
      "typing-extensions-3. | 8 KB      | ########## | 100% \n",
      "aiobotocore-1.1.2    | 39 KB     | ########## | 100% \n",
      "jmespath-0.10.0      | 21 KB     | ########## | 100% \n",
      "aiohttp-3.7.3        | 619 KB    | ########## | 100% \n",
      "wrapt-1.12.1         | 47 KB     | ########## | 100% \n",
      "conda-4.9.2          | 3.0 MB    | ########## | 100% \n",
      "aioitertools-0.7.1   | 19 KB     | ########## | 100% \n",
      "attrs-20.3.0         | 41 KB     | ########## | 100% \n",
      "yarl-1.6.3           | 142 KB    | ########## | 100% \n",
      "certifi-2020.11.8    | 150 KB    | ########## | 100% \n",
      "docutils-0.15.2      | 735 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Removing intermediate container 81d75e78da03\n",
      " ---> cf023469ae11\n",
      "Step 17/21 : RUN mkdir /opt/app /etc/dask\n",
      " ---> Running in 16ba66f8fd39\n",
      "Removing intermediate container 16ba66f8fd39\n",
      " ---> 5982742d684b\n",
      "Step 18/21 : COPY dask_config/dask.yaml /etc/dask/\n",
      " ---> 4250f8097da3\n",
      "Step 19/21 : COPY program /opt/program\n",
      " ---> 56a42af1d301\n",
      "Step 20/21 : RUN chmod +x /opt/program/bootstrap.py\n",
      " ---> Running in 939a9bff3a20\n",
      "Removing intermediate container 939a9bff3a20\n",
      " ---> 702debc5748c\n",
      "Step 21/21 : ENTRYPOINT [\"/opt/program/bootstrap.py\"]\n",
      " ---> Running in 58a846b50af7\n",
      "Removing intermediate container 58a846b50af7\n",
      " ---> d85056ae810f\n",
      "Successfully built d85056ae810f\n",
      "Successfully tagged sagemaker-dask-example:latest\n",
      "/home/ec2-user/SageMaker/feature_transformation_with_sagemaker_processing_dask_2020-10-29\n"
     ]
    }
   ],
   "source": [
    "%cd container\n",
    "!docker build -t sagemaker-dask-example .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Dask container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'sagemaker-dask-example' already exists in the registry with id '571572254388'\n",
      "The push refers to repository [571572254388.dkr.ecr.us-east-1.amazonaws.com/sagemaker-dask-example]\n",
      "\n",
      "\u001b[1B24ad56be: Preparing \n",
      "\u001b[1Bab3ab5f4: Preparing \n",
      "\u001b[1B3b52e4ad: Preparing \n",
      "\u001b[1Bb0b0f24b: Preparing \n",
      "\u001b[1Baa92540f: Preparing \n",
      "\u001b[1B4a50d40f: Preparing \n",
      "\u001b[1B284e419d: Preparing \n",
      "\u001b[1Bbdadd1e1: Preparing \n",
      "\u001b[1B1120ec98: Preparing \n",
      "\u001b[1Beaf0bdec: Preparing \n",
      "\u001b[1B6ba85e14: Preparing \n",
      "\u001b[1B1fe7c5e4: Preparing \n",
      "\u001b[1B3850fda1: Preparing \n",
      "\u001b[1B9805207b: Preparing \n",
      "\u001b[1Bbed7025d: Preparing \n",
      "\u001b[1B081ac115: Preparing \n",
      "\u001b[1Bcb249b79: Preparing \n",
      "\u001b[1B190fd43a: Preparing \n",
      "\u001b[1B4bce66cd: Layer already exists \u001b[18A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2Klatest: digest: sha256:6ab6b31d5def58ccf7184e50b7b031ecee7ed272b5a99024d7cea5eda79e56e1 size: 4308\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'sagemaker-dask-example'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "dask_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $dask_repository_uri\n",
    "!docker push $dask_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing job using Amazon SageMaker Processing on Dask Cluster\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the the custom Dask container that was just built, and a Scikit Learn script for preprocessing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dask preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from __future__ import print_function, unicode_literals\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tornado import gen\n",
    "import dask.dataframe as dd\n",
    "import joblib\n",
    "from dask.distributed import Client\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    KBinsDiscretizer,\n",
    "    LabelBinarizer,\n",
    "    OneHotEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
    "attempts_counter = 3\n",
    "attempts = 0\n",
    "\n",
    "\n",
    "def upload_objects(bucket, prefix, local_path):\n",
    "    try:\n",
    "        bucket_name = bucket  # s3 bucket name\n",
    "        root_path = local_path  # local folder for upload\n",
    "\n",
    "        s3_bucket = s3_client.Bucket(bucket_name)\n",
    "\n",
    "        for path, subdirs, files in os.walk(root_path):\n",
    "            for file in files:\n",
    "                s3_bucket.upload_file(\n",
    "                    os.path.join(path, file), \"{}/output/{}\".format(prefix, file)\n",
    "                )\n",
    "    except Exception as err:\n",
    "        logging.exception(err)\n",
    "\n",
    "\n",
    "def print_shape(df):\n",
    "    negative_examples, positive_examples = np.bincount(df[\"income\"])\n",
    "    print(\n",
    "        \"Data shape: {}, {} positive examples, {} negative examples\".format(\n",
    "            df.shape, positive_examples, negative_examples\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Get processor scrip arguments\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    script_args = dict(zip(args_iter, args_iter))\n",
    "    scheduler_ip = sys.argv[-1]\n",
    "\n",
    "    # S3 client\n",
    "    s3_region = script_args[\"s3_region\"]\n",
    "    s3_client = boto3.resource(\"s3\", s3_region)\n",
    "    print(f'Using the {s3_region} region')\n",
    "    \n",
    "    # Start the Dask cluster client\n",
    "    try:\n",
    "        client = Client(\"tcp://{ip}:8786\".format(ip=scheduler_ip))\n",
    "        logging.info(\"Printing cluster information: {}\".format(client))\n",
    "    except Exception as err:\n",
    "        logging.exception(err)\n",
    "\n",
    "    columns = [\n",
    "        \"age\",\n",
    "        \"education\",\n",
    "        \"major industry code\",\n",
    "        \"class of worker\",\n",
    "        \"num persons worked for employer\",\n",
    "        \"capital gains\",\n",
    "        \"capital losses\",\n",
    "        \"dividends from stocks\",\n",
    "        \"income\",\n",
    "    ]\n",
    "    class_labels = [\" - 50000.\", \" 50000+.\"]\n",
    "    input_data_path = \"s3://{}\".format(os.path.join(\n",
    "        script_args[\"s3_input_bucket\"],\n",
    "        script_args[\"s3_input_key_prefix\"],\n",
    "        \"census-income.csv\",\n",
    "    ))\n",
    "    \n",
    "    # Creating the necessary paths to save the output files\n",
    "    if not os.path.exists(\"/opt/ml/processing/train\"):\n",
    "        os.makedirs(\"/opt/ml/processing/train\")\n",
    "\n",
    "    if not os.path.exists(\"/opt/ml/processing/test\"):\n",
    "        os.makedirs(\"/opt/ml/processing/test\")\n",
    "\n",
    "    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df = pd.DataFrame(data=df, columns=columns)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.replace(class_labels, [0, 1], inplace=True)\n",
    "\n",
    "    negative_examples, positive_examples = np.bincount(df[\"income\"])\n",
    "    print(\n",
    "        \"Data after cleaning: {}, {} positive examples, {} negative examples\".format(\n",
    "            df.shape, positive_examples, negative_examples\n",
    "        )\n",
    "    )\n",
    "\n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    print(\"Splitting data into train and test sets with ratio {}\".format(split_ratio))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.drop(\"income\", axis=1), df[\"income\"], test_size=split_ratio, random_state=0\n",
    "    )\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        (\n",
    "            KBinsDiscretizer(encode=\"onehot-dense\", n_bins=2),\n",
    "            [\"age\", \"num persons worked for employer\"],\n",
    "        ),\n",
    "        (\n",
    "            StandardScaler(),\n",
    "            [\"capital gains\", \"capital losses\", \"dividends from stocks\"],\n",
    "        ),\n",
    "        (\n",
    "            OneHotEncoder(sparse=False),\n",
    "            [\"education\", \"major industry code\", \"class of worker\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(\"Running preprocessing and feature engineering transformations in Dask\")\n",
    "    with joblib.parallel_backend(\"dask\"):\n",
    "        train_features = preprocess.fit_transform(X_train)\n",
    "        test_features = preprocess.transform(X_test)\n",
    "\n",
    "    print(\"Train data shape after preprocessing: {}\".format(train_features.shape))\n",
    "    print(\"Test data shape after preprocessing: {}\".format(test_features.shape))\n",
    "\n",
    "    train_features_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_features.csv\"\n",
    "    )\n",
    "    train_labels_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/train\", \"train_labels.csv\"\n",
    "    )\n",
    "\n",
    "    test_features_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/test\", \"test_features.csv\"\n",
    "    )\n",
    "    test_labels_output_path = os.path.join(\"/opt/ml/processing/test\", \"test_labels.csv\")\n",
    "\n",
    "    print(\"Saving training features to {}\".format(train_features_output_path))\n",
    "    pd.DataFrame(train_features).to_csv(\n",
    "        train_features_output_path, header=False, index=False\n",
    "    )\n",
    "\n",
    "    print(\"Saving test features to {}\".format(test_features_output_path))\n",
    "    pd.DataFrame(test_features).to_csv(\n",
    "        test_features_output_path, header=False, index=False\n",
    "    )\n",
    "\n",
    "    print(\"Saving training labels to {}\".format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "\n",
    "    print(\"Saving test labels to {}\".format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n",
    "    upload_objects(\n",
    "        script_args[\"s3_output_bucket\"],\n",
    "        script_args[\"s3_output_key_prefix\"],\n",
    "        \"/opt/ml/processing/train/\",\n",
    "    )\n",
    "    upload_objects(\n",
    "        script_args[\"s3_output_bucket\"],\n",
    "        script_args[\"s3_output_key_prefix\"],\n",
    "        \"/opt/ml/processing/test/\",\n",
    "    )\n",
    "\n",
    "    # wait for the file creation\n",
    "    while attempts < attempts_counter:\n",
    "        if os.path.exists(train_features_output_path) and os.path.isfile(\n",
    "            train_features_output_path\n",
    "        ):\n",
    "            try:\n",
    "                # Calculate the processed dataset baseline statistics on the Dask cluster\n",
    "                dask_df = dd.read_csv(train_features_output_path)\n",
    "                dask_df = client.persist(dask_df)\n",
    "                baseline = dask_df.describe().compute()\n",
    "                print(baseline)\n",
    "                break\n",
    "\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "    if attempts == attempts_counter:\n",
    "        raise Exception(\n",
    "            \"Output file {} couldn't be found\".format(train_features_output_path)\n",
    "        )\n",
    "\n",
    "    print(client)\n",
    "    sys.exit(os.EX_OK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a processing job using the Docker image and preprocessing script you just created. When invoking the `dask_processor.run()` function, pass the Amazon S3 input and output paths as arguments that are required by our preprocessing script to determine input and output location in Amazon S3. Here, you also specify the number of instances and instance type that will be used for the distributed Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  dask-preprocessor-2020-11-19-19-22-08-782\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-571572254388/dask-preprocessor-2020-11-19-19-22-08-782/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      ".........................\u001b[35mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.190.180:43073'\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.156.191:44341'\u001b[0m\n",
      "\u001b[34mdistributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Start worker at:   tcp://10.0.190.180:44809\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          Listening to:   tcp://10.0.190.180:44809\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          dashboard at:         10.0.190.180:40945\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.156.191:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -                Memory:                    6.52 GB\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-ompbw8en\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Clear task state\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   Scheduler at:   tcp://10.0.156.191:8786\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   dashboard at:                     :8787\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Start worker at:   tcp://10.0.156.191:42665\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          Listening to:   tcp://10.0.156.191:42665\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          dashboard at:         10.0.156.191:46445\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.156.191:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -                Memory:                    6.62 GB\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-k6d2l2zw\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.156.191:42665', name: tcp://10.0.156.191:42665, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.156.191:42665\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -         Registered to:    tcp://10.0.156.191:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.190.180:44809', name: tcp://10.0.190.180:44809, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.190.180:44809\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -         Registered to:    tcp://10.0.156.191:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Receive client connection: Client-1b86969c-2a9d-11eb-801a-9a345d7497d9\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Receive client connection: Client-worker-202a5a2f-2a9d-11eb-800f-a98c93bd0329\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Receive client connection: Client-worker-2028ec78-2a9d-11eb-800f-a98c93bd0329\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Receive client connection: Client-worker-203c3adf-2a9d-11eb-800d-7a93e787cd50\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[35mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[34mUsing the us-east-1 region\u001b[0m\n",
      "\u001b[34mReading input data from s3://sagemaker-us-east-1-571572254388/sagemaker/dask-preprocess-demo/input/raw/census/census-income.csv\u001b[0m\n",
      "\u001b[34mData after cleaning: (68285, 9), 11401 positive examples, 56884 negative examples\u001b[0m\n",
      "\u001b[34mSplitting data into train and test sets with ratio 0.3\u001b[0m\n",
      "\u001b[34mRunning preprocessing and feature engineering transformations in Dask\u001b[0m\n",
      "\u001b[34mTrain data shape after preprocessing: (47799, 57)\u001b[0m\n",
      "\u001b[34mTest data shape after preprocessing: (20486, 57)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\u001b[34mSaving test features to /opt/ml/processing/test/test_features.csv\u001b[0m\n",
      "\u001b[34mSaving training labels to /opt/ml/processing/train/train_labels.csv\u001b[0m\n",
      "\u001b[34mSaving test labels to /opt/ml/processing/test/test_labels.csv\n",
      "                1.0           0.0  ...        0.0.47        0.0.48\u001b[0m\n",
      "\u001b[34mcount  47798.000000  47798.000000  ...  47798.000000  47798.000000\u001b[0m\n",
      "\u001b[34mmean       0.498598      0.501402  ...      0.042993      0.002260\u001b[0m\n",
      "\u001b[34mstd        0.500003      0.500003  ...      0.202844      0.047481\u001b[0m\n",
      "\u001b[34mmin        0.000000      0.000000  ...      0.000000      0.000000\u001b[0m\n",
      "\u001b[34m25%        0.000000      0.000000  ...      0.000000      0.000000\u001b[0m\n",
      "\u001b[34m50%        0.000000      1.000000  ...      0.000000      0.000000\u001b[0m\n",
      "\u001b[34m75%        1.000000      1.000000  ...      0.000000      0.000000\u001b[0m\n",
      "\u001b[34mmax        1.000000      1.000000  ...      1.000000      1.000000\n",
      "\u001b[0m\n",
      "\u001b[34m[8 rows x 57 columns]\u001b[0m\n",
      "\u001b[34m<Client: 'tcp://10.0.156.191:8786' processes=2 threads=4, memory=13.15 GB>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-1b86969c-2a9d-11eb-801a-9a345d7497d9\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-1b86969c-2a9d-11eb-801a-9a345d7497d9\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Close client connection: Client-1b86969c-2a9d-11eb-801a-9a345d7497d9\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ScriptProcessor\n",
    "\n",
    "dask_processor = ScriptProcessor(\n",
    "    base_job_name=\"dask-preprocessor\",\n",
    "    image_uri=dask_repository_uri,\n",
    "    command=[\"/opt/program/bootstrap.py\"],\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "dask_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    arguments=[\n",
    "        \"s3_input_bucket\",\n",
    "        bucket,\n",
    "        \"s3_input_key_prefix\",\n",
    "        input_prefix,\n",
    "        \"s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"s3_output_key_prefix\",\n",
    "        input_preprocessed_prefix,\n",
    "        \"s3_region\",\n",
    "        region\n",
    "    ],\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the preprocessed dataset\n",
    "Take a look at a few rows of the transformed dataset to make sure the preprocessing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 5 rows from s3://{}/{}/train/'.format(bucket, input_preprocessed_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix/output/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the output files of the transformation process as input to a training job and train a regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
